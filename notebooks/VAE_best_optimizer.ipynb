{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "\n",
    "def convert_sample(sample):\n",
    "    image, label = sample['image'], sample['label']  \n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    return image, image  # Use the image itself as the target label\n",
    "\n",
    "def map_func(sample):\n",
    "    image = sample['image']\n",
    "    label = sample['label']\n",
    "    label = tf.one_hot(label, 2, dtype=tf.float32)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "# Corrected file path with escaped backslashes and a missing comma\n",
    "ds1, ds2, ds3 = tfds.load('patch_camelyon',\n",
    "                         split=['train[:20%]', 'test[:5%]', 'validation[:5%]'],\n",
    "                         data_dir=r'C:\\Users\\jeppe\\OneDrive\\Dokumenter\\Privat\\SDU\\DataScience\\Anvendtmaskinlæring\\eksamen',\n",
    "                         download=False,\n",
    "                         shuffle_files=True)\n",
    "\n",
    "train_dataset       = ds1.map(convert_sample).batch(32)\n",
    "validation_dataset  = ds3.map(convert_sample).batch(32)\n",
    "test_dataset        = ds2.map(convert_sample).batch(32)\n",
    "\n",
    "train_dataset_cnn       = ds1.map(map_func).batch(32)\n",
    "validation_dataset_cnn  = ds3.map(map_func).batch(32)\n",
    "test_dataset_cnn        = ds2.map(map_func).batch(32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bedste optimizer loop\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam, SGD, Adagrad, Adadelta, Adamax, Nadam, RMSprop\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Your provided VAE classes (Encoder, Decoder, VAE) go here\n",
    "\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "\n",
    "# Define the encoder part\n",
    "class Encoder(tf.keras.layers.Layer): #initialiserer encoderen, definerer lagene og præciserer hvordan vores model skal konstrueres\n",
    "    def __init__(self, latent_dim): \n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')\n",
    "        self.maxpool1 = layers.MaxPooling2D((2, 2), padding='same')\n",
    "        self.conv2 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')\n",
    "        self.maxpool2 = layers.MaxPooling2D((2, 2), padding='same')\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.dense_mean = layers.Dense(latent_dim)\n",
    "        self.dense_log_var = layers.Dense(latent_dim)\n",
    "\n",
    "    def call(self, x): #den faktiske model der forward pass'er data og transformerer det på baggrund af hvad der er defineret i def _init_\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.flatten(x)\n",
    "        mean = self.dense_mean(x)\n",
    "        log_var = self.dense_log_var(x)\n",
    "        return mean, log_var\n",
    "\n",
    "# Define the decoder part\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, latent_dim, original_shape):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dense = layers.Dense(units=original_shape[0] * original_shape[1] * original_shape[2], activation=tf.nn.relu)\n",
    "        self.reshape = layers.Reshape(target_shape=(original_shape[0], original_shape[1], original_shape[2]))\n",
    "        self.conv2dtranspose1 = layers.Conv2DTranspose(64, (3, 3), activation='relu', padding='same')\n",
    "        self.conv2dtranspose2 = layers.Conv2DTranspose(32, (3, 3), activation='relu', padding='same')\n",
    "        self.conv2dtranspose3 = layers.Conv2DTranspose(3, (3, 3), activation='sigmoid', padding='same')\n",
    "\n",
    "    def call(self, z):\n",
    "        z = self.dense(z)\n",
    "        z = self.reshape(z)\n",
    "        z = self.conv2dtranspose1(z)\n",
    "        z = self.conv2dtranspose2(z)\n",
    "        reconstructed = self.conv2dtranspose3(z)\n",
    "        return reconstructed\n",
    "\n",
    "# Define the Variational Autoencoder as a whole\n",
    "class VAE(Model):\n",
    "    def __init__(self, latent_dim, original_shape):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, original_shape)\n",
    "\n",
    "    def sample(self, mean, log_var):\n",
    "        batch = tf.shape(mean)[0]\n",
    "        dim = tf.shape(mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return mean + tf.exp(0.5 * log_var) * epsilon\n",
    "\n",
    "    def call(self, x):\n",
    "        mean, log_var = self.encoder(x)\n",
    "        z = self.sample(mean, log_var)\n",
    "        reconstructed = self.decoder(z)\n",
    "        return reconstructed\n",
    "\n",
    "# Set the latent dimension and original image shape\n",
    "latent_dim = 128\n",
    "original_shape = (96, 96, 3)  # Assuming PCAM images are of shape 96x96x3\n",
    "\n",
    "# List of optimizers to compare\n",
    "optimizers = ['sgd', 'adam', 'rmsprop', 'adagrad', 'adamax', 'nadam']\n",
    "\n",
    "# Plot colors for each optimizer\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y']\n",
    "\n",
    "# Initialize subplots\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for i, optimizer in enumerate(optimizers):\n",
    "    # Instantiate the VAE\n",
    "    vae = VAE(latent_dim, original_shape)\n",
    "    \n",
    "     # Freeze the layers in the encoder\n",
    "    vae.encoder.conv1.trainable = False\n",
    "    vae.encoder.maxpool1.trainable = False\n",
    "    vae.encoder.conv2.trainable = False\n",
    "    vae.encoder.maxpool2.trainable = False\n",
    "    vae.encoder.flatten.trainable = False\n",
    "    vae.encoder.dense_mean.trainable = False\n",
    "    vae.encoder.dense_log_var.trainable = False\n",
    "\n",
    "    # Compile the model with the current optimizer\n",
    "    vae.compile(optimizer=optimizer, loss=MeanSquaredError())\n",
    "\n",
    "    # Train the VAE\n",
    "    history = vae.fit(\n",
    "        train_dataset,\n",
    "        epochs=5,  # You can adjust the number of epochs\n",
    "        batch_size=256,\n",
    "        shuffle=True,\n",
    "        validation_data=validation_dataset,\n",
    "        callbacks=[TensorBoard(log_dir=f'/tmp/vae_{optimizer}'), EarlyStopping(monitor='val_loss', patience=3)]\n",
    "    )\n",
    "\n",
    "    # Access the training history\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    # Plot the training progress for the current optimizer\n",
    "    plt.plot(loss, label=f'Training Loss ({optimizer})', linestyle='-', color=colors[i])\n",
    "    plt.plot(val_loss, label=f'Validation Loss ({optimizer})', linestyle='--', color=colors[i])\n",
    "\n",
    "# Set plot labels and legend\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
