{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3077, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3132, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3336, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3519, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3579, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\jeppe\\AppData\\Local\\Temp\\ipykernel_14952\\2817586842.py\", line 1, in <module>\n",
      "    import tensorflow_datasets as tfds\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\tensorflow_datasets\\__init__.py\", line 43, in <module>\n",
      "    import tensorflow_datasets.core.logging as _tfds_logging\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\tensorflow_datasets\\core\\__init__.py\", line 22, in <module>\n",
      "    from tensorflow_datasets.core import community\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\tensorflow_datasets\\core\\community\\__init__.py\", line 18, in <module>\n",
      "    from tensorflow_datasets.core.community.huggingface_wrapper import mock_builtin_to_use_gfile\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\tensorflow_datasets\\core\\community\\huggingface_wrapper.py\", line 31, in <module>\n",
      "    from tensorflow_datasets.core import dataset_builder\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py\", line 33, in <module>\n",
      "    from tensorflow_datasets.core import dataset_info\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_info.py\", line 51, in <module>\n",
      "    from tensorflow_datasets.core.features import feature as feature_lib\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\tensorflow_datasets\\core\\features\\__init__.py\", line 26, in <module>\n",
      "    from tensorflow_datasets.core.features.dataset_feature import Dataset\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\tensorflow_datasets\\core\\features\\dataset_feature.py\", line 23, in <module>\n",
      "    from tensorflow_datasets.core.features import sequence_feature\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\tensorflow_datasets\\core\\features\\sequence_feature.py\", line 25, in <module>\n",
      "    from tensorflow_datasets.core.features import features_dict\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\tensorflow_datasets\\core\\features\\features_dict.py\", line 26, in <module>\n",
      "    from tensorflow_datasets.core.features import top_level_feature\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\tensorflow_datasets\\core\\features\\top_level_feature.py\", line 23, in <module>\n",
      "    from tensorflow_datasets.core import example_parser\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\tensorflow_datasets\\core\\example_parser.py\", line 31, in <module>\n",
      "    example_pb2 = tf.train\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\tensorflow_datasets\\core\\utils\\lazy_imports_utils.py\", line 68, in __getattr__\n",
      "    self.module = importlib.import_module(self.module_name)\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\tensorflow\\__init__.py\", line 468, in <module>\n",
      "    importlib.import_module(\"keras.src.optimizers\")\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\keras\\__init__.py\", line 7, in <module>\n",
      "    from keras import _tf_keras as _tf_keras\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\keras\\_tf_keras\\__init__.py\", line 1, in <module>\n",
      "    from keras._tf_keras import keras\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\keras\\_tf_keras\\keras\\__init__.py\", line 7, in <module>\n",
      "    from keras import activations as activations\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\keras\\activations\\__init__.py\", line 7, in <module>\n",
      "    from keras.src.activations import deserialize as deserialize\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\keras\\src\\__init__.py\", line 1, in <module>\n",
      "    from keras.src import activations\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\keras\\src\\activations\\__init__.py\", line 34, in <module>\n",
      "    from keras.src.saving import object_registration\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\keras\\src\\saving\\__init__.py\", line 7, in <module>\n",
      "    from keras.src.saving.saving_api import load_model\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\keras\\src\\saving\\saving_api.py\", line 7, in <module>\n",
      "    from keras.src.legacy.saving import legacy_h5_format\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\keras\\src\\legacy\\saving\\legacy_h5_format.py\", line 13, in <module>\n",
      "    from keras.src.legacy.saving import saving_utils\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\keras\\src\\legacy\\saving\\saving_utils.py\", line 10, in <module>\n",
      "    from keras.src import models\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\keras\\src\\models\\__init__.py\", line 1, in <module>\n",
      "    from keras.src.models.functional import Functional\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\keras\\src\\models\\functional.py\", line 16, in <module>\n",
      "    from keras.src.models.model import Model\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\keras\\src\\models\\model.py\", line 12, in <module>\n",
      "    from keras.src.trainers import trainer as base_trainer\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\keras\\src\\trainers\\trainer.py\", line 14, in <module>\n",
      "    from keras.src.trainers.data_adapters import data_adapter_utils\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\__init__.py\", line 4, in <module>\n",
      "    from keras.src.trainers.data_adapters import array_data_adapter\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\array_data_adapter.py\", line 7, in <module>\n",
      "    from keras.src.trainers.data_adapters import array_slicing\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\array_slicing.py\", line 12, in <module>\n",
      "    import pandas\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\pandas\\__init__.py\", line 26, in <module>\n",
      "    from pandas.compat import (\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\pandas\\compat\\__init__.py\", line 27, in <module>\n",
      "    from pandas.compat.pyarrow import (\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\pandas\\compat\\pyarrow.py\", line 8, in <module>\n",
      "    import pyarrow as pa\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\pyarrow\\__init__.py\", line 65, in <module>\n",
      "    import pyarrow.lib as _lib\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3077, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3132, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3336, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3519, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3579, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\jeppe\\AppData\\Local\\Temp\\ipykernel_14952\\2817586842.py\", line 1, in <module>\n",
      "    import tensorflow_datasets as tfds\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\tensorflow_datasets\\__init__.py\", line 43, in <module>\n",
      "    import tensorflow_datasets.core.logging as _tfds_logging\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\tensorflow_datasets\\core\\__init__.py\", line 22, in <module>\n",
      "    from tensorflow_datasets.core import community\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\tensorflow_datasets\\core\\community\\__init__.py\", line 18, in <module>\n",
      "    from tensorflow_datasets.core.community.huggingface_wrapper import mock_builtin_to_use_gfile\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\tensorflow_datasets\\core\\community\\huggingface_wrapper.py\", line 31, in <module>\n",
      "    from tensorflow_datasets.core import dataset_builder\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py\", line 33, in <module>\n",
      "    from tensorflow_datasets.core import dataset_info\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_info.py\", line 51, in <module>\n",
      "    from tensorflow_datasets.core.features import feature as feature_lib\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\tensorflow_datasets\\core\\features\\__init__.py\", line 26, in <module>\n",
      "    from tensorflow_datasets.core.features.dataset_feature import Dataset\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\tensorflow_datasets\\core\\features\\dataset_feature.py\", line 23, in <module>\n",
      "    from tensorflow_datasets.core.features import sequence_feature\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\tensorflow_datasets\\core\\features\\sequence_feature.py\", line 25, in <module>\n",
      "    from tensorflow_datasets.core.features import features_dict\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\tensorflow_datasets\\core\\features\\features_dict.py\", line 26, in <module>\n",
      "    from tensorflow_datasets.core.features import top_level_feature\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\tensorflow_datasets\\core\\features\\top_level_feature.py\", line 23, in <module>\n",
      "    from tensorflow_datasets.core import example_parser\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\tensorflow_datasets\\core\\example_parser.py\", line 31, in <module>\n",
      "    example_pb2 = tf.train\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\tensorflow_datasets\\core\\utils\\lazy_imports_utils.py\", line 68, in __getattr__\n",
      "    self.module = importlib.import_module(self.module_name)\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\tensorflow\\__init__.py\", line 468, in <module>\n",
      "    importlib.import_module(\"keras.src.optimizers\")\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\keras\\__init__.py\", line 7, in <module>\n",
      "    from keras import _tf_keras as _tf_keras\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\keras\\_tf_keras\\__init__.py\", line 1, in <module>\n",
      "    from keras._tf_keras import keras\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\keras\\_tf_keras\\keras\\__init__.py\", line 7, in <module>\n",
      "    from keras import activations as activations\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\keras\\activations\\__init__.py\", line 7, in <module>\n",
      "    from keras.src.activations import deserialize as deserialize\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\keras\\src\\__init__.py\", line 1, in <module>\n",
      "    from keras.src import activations\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\keras\\src\\activations\\__init__.py\", line 34, in <module>\n",
      "    from keras.src.saving import object_registration\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\keras\\src\\saving\\__init__.py\", line 7, in <module>\n",
      "    from keras.src.saving.saving_api import load_model\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\keras\\src\\saving\\saving_api.py\", line 7, in <module>\n",
      "    from keras.src.legacy.saving import legacy_h5_format\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\keras\\src\\legacy\\saving\\legacy_h5_format.py\", line 13, in <module>\n",
      "    from keras.src.legacy.saving import saving_utils\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\keras\\src\\legacy\\saving\\saving_utils.py\", line 10, in <module>\n",
      "    from keras.src import models\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\keras\\src\\models\\__init__.py\", line 1, in <module>\n",
      "    from keras.src.models.functional import Functional\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\keras\\src\\models\\functional.py\", line 16, in <module>\n",
      "    from keras.src.models.model import Model\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\keras\\src\\models\\model.py\", line 12, in <module>\n",
      "    from keras.src.trainers import trainer as base_trainer\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\keras\\src\\trainers\\trainer.py\", line 14, in <module>\n",
      "    from keras.src.trainers.data_adapters import data_adapter_utils\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\__init__.py\", line 4, in <module>\n",
      "    from keras.src.trainers.data_adapters import array_data_adapter\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\array_data_adapter.py\", line 7, in <module>\n",
      "    from keras.src.trainers.data_adapters import array_slicing\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\array_slicing.py\", line 12, in <module>\n",
      "    import pandas\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\pandas\\__init__.py\", line 49, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\pandas\\core\\api.py\", line 9, in <module>\n",
      "    from pandas.core.dtypes.dtypes import (\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\pandas\\core\\dtypes\\dtypes.py\", line 24, in <module>\n",
      "    from pandas._libs import (\n",
      "  File \"c:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\pyarrow\\__init__.py\", line 65, in <module>\n",
      "    import pyarrow.lib as _lib\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error parsing split 'test[:2.5%]'. See format at: https://www.tensorflow.org/datasets/splits\nUnrecognized split format: 'test[:2.5%]'. See format at https://www.tensorflow.org/datasets/splits",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 17\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image, label\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Corrected file path with escaped backslashes and a missing comma\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m ds1, ds2, ds3 \u001b[38;5;241m=\u001b[39m \u001b[43mtfds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpatch_camelyon\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m                         \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain[:10\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest[:2.5\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalidation[:2.5\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mJob_og_eksamensbevis\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mGithub\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mprojekter\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mProjekt_1_Computer_Vision\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mpath_to_data_directory\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mshuffle_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m train_dataset       \u001b[38;5;241m=\u001b[39m ds1\u001b[38;5;241m.\u001b[39mmap(convert_sample)\u001b[38;5;241m.\u001b[39mbatch(\u001b[38;5;241m32\u001b[39m)\n\u001b[0;32m     24\u001b[0m validation_dataset  \u001b[38;5;241m=\u001b[39m ds3\u001b[38;5;241m.\u001b[39mmap(convert_sample)\u001b[38;5;241m.\u001b[39mbatch(\u001b[38;5;241m32\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py:169\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[1;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_call()\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    171\u001b[0m   metadata\u001b[38;5;241m.\u001b[39mmark_error()\n",
      "File \u001b[1;32mc:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\tensorflow_datasets\\core\\load.py:629\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[0;32m    626\u001b[0m as_dataset_kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshuffle_files\u001b[39m\u001b[38;5;124m'\u001b[39m, shuffle_files)\n\u001b[0;32m    627\u001b[0m as_dataset_kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread_config\u001b[39m\u001b[38;5;124m'\u001b[39m, read_config)\n\u001b[1;32m--> 629\u001b[0m ds \u001b[38;5;241m=\u001b[39m dbuilder\u001b[38;5;241m.\u001b[39mas_dataset(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mas_dataset_kwargs)\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_info:\n\u001b[0;32m    631\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m ds, dbuilder\u001b[38;5;241m.\u001b[39minfo\n",
      "File \u001b[1;32mc:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py:169\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[1;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_call()\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    171\u001b[0m   metadata\u001b[38;5;241m.\u001b[39mmark_error()\n",
      "File \u001b[1;32mc:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:781\u001b[0m, in \u001b[0;36mDatasetBuilder.as_dataset\u001b[1;34m(self, split, batch_size, shuffle_files, decoders, read_config, as_supervised)\u001b[0m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;66;03m# Create a dataset for each of the given splits\u001b[39;00m\n\u001b[0;32m    773\u001b[0m build_single_dataset \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[0;32m    774\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_single_dataset,\n\u001b[0;32m    775\u001b[0m     shuffle_files\u001b[38;5;241m=\u001b[39mshuffle_files,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    779\u001b[0m     as_supervised\u001b[38;5;241m=\u001b[39mas_supervised,\n\u001b[0;32m    780\u001b[0m )\n\u001b[1;32m--> 781\u001b[0m all_ds \u001b[38;5;241m=\u001b[39m \u001b[43mtree_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuild_single_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    782\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m all_ds\n",
      "File \u001b[1;32mc:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\tree\\__init__.py:430\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structures, **kwargs)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m structures[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m    428\u001b[0m   assert_same_structure(structures[\u001b[38;5;241m0\u001b[39m], other, check_types\u001b[38;5;241m=\u001b[39mcheck_types)\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unflatten_as(structures[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m--> 430\u001b[0m                     [func(\u001b[38;5;241m*\u001b[39margs) \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(flatten, structures))])\n",
      "File \u001b[1;32mc:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\tree\\__init__.py:430\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m structures[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m    428\u001b[0m   assert_same_structure(structures[\u001b[38;5;241m0\u001b[39m], other, check_types\u001b[38;5;241m=\u001b[39mcheck_types)\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unflatten_as(structures[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m--> 430\u001b[0m                     [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(flatten, structures))])\n",
      "File \u001b[1;32mc:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:799\u001b[0m, in \u001b[0;36mDatasetBuilder._build_single_dataset\u001b[1;34m(self, split, batch_size, shuffle_files, decoders, read_config, as_supervised)\u001b[0m\n\u001b[0;32m    796\u001b[0m   batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mtotal_num_examples \u001b[38;5;129;01mor\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mmaxsize\n\u001b[0;32m    798\u001b[0m \u001b[38;5;66;03m# Build base dataset\u001b[39;00m\n\u001b[1;32m--> 799\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_as_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mread_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    805\u001b[0m \u001b[38;5;66;03m# Auto-cache small datasets which are small enough to fit in memory.\u001b[39;00m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_cache_ds(\n\u001b[0;32m    807\u001b[0m     split\u001b[38;5;241m=\u001b[39msplit, shuffle_files\u001b[38;5;241m=\u001b[39mshuffle_files, read_config\u001b[38;5;241m=\u001b[39mread_config\n\u001b[0;32m    808\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:1252\u001b[0m, in \u001b[0;36mFileReaderBuilder._as_dataset\u001b[1;34m(self, split, decoders, read_config, shuffle_files)\u001b[0m\n\u001b[0;32m   1246\u001b[0m reader \u001b[38;5;241m=\u001b[39m reader_lib\u001b[38;5;241m.\u001b[39mReader(\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_dir,\n\u001b[0;32m   1248\u001b[0m     example_specs\u001b[38;5;241m=\u001b[39mexample_specs,\n\u001b[0;32m   1249\u001b[0m     file_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mfile_format,\n\u001b[0;32m   1250\u001b[0m )\n\u001b[0;32m   1251\u001b[0m decode_fn \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(features\u001b[38;5;241m.\u001b[39mdecode_example, decoders\u001b[38;5;241m=\u001b[39mdecoders)\n\u001b[1;32m-> 1252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1253\u001b[0m \u001b[43m    \u001b[49m\u001b[43minstructions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1254\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_infos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mread_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_shuffling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_shuffling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\tensorflow_datasets\\core\\reader.py:413\u001b[0m, in \u001b[0;36mReader.read\u001b[1;34m(self, instructions, split_infos, read_config, shuffle_files, disable_shuffling, decode_fn)\u001b[0m\n\u001b[0;32m    404\u001b[0m   file_instructions \u001b[38;5;241m=\u001b[39m splits_dict[instruction]\u001b[38;5;241m.\u001b[39mfile_instructions\n\u001b[0;32m    405\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_files(\n\u001b[0;32m    406\u001b[0m       file_instructions,\n\u001b[0;32m    407\u001b[0m       read_config\u001b[38;5;241m=\u001b[39mread_config,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    410\u001b[0m       decode_fn\u001b[38;5;241m=\u001b[39mdecode_fn,\n\u001b[0;32m    411\u001b[0m   )\n\u001b[1;32m--> 413\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtree_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_read_instruction_to_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\tree\\__init__.py:430\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structures, **kwargs)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m structures[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m    428\u001b[0m   assert_same_structure(structures[\u001b[38;5;241m0\u001b[39m], other, check_types\u001b[38;5;241m=\u001b[39mcheck_types)\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unflatten_as(structures[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m--> 430\u001b[0m                     [func(\u001b[38;5;241m*\u001b[39margs) \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(flatten, structures))])\n",
      "File \u001b[1;32mc:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\tree\\__init__.py:430\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m structures[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m    428\u001b[0m   assert_same_structure(structures[\u001b[38;5;241m0\u001b[39m], other, check_types\u001b[38;5;241m=\u001b[39mcheck_types)\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unflatten_as(structures[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m--> 430\u001b[0m                     [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(flatten, structures))])\n",
      "File \u001b[1;32mc:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\tensorflow_datasets\\core\\reader.py:404\u001b[0m, in \u001b[0;36mReader.read.<locals>._read_instruction_to_ds\u001b[1;34m(instruction)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_instruction_to_ds\u001b[39m(instruction):\n\u001b[1;32m--> 404\u001b[0m   file_instructions \u001b[38;5;241m=\u001b[39m \u001b[43msplits_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43minstruction\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mfile_instructions\n\u001b[0;32m    405\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_files(\n\u001b[0;32m    406\u001b[0m       file_instructions,\n\u001b[0;32m    407\u001b[0m       read_config\u001b[38;5;241m=\u001b[39mread_config,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    410\u001b[0m       decode_fn\u001b[38;5;241m=\u001b[39mdecode_fn,\n\u001b[0;32m    411\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\tensorflow_datasets\\core\\splits.py:400\u001b[0m, in \u001b[0;36mSplitDict.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    397\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SplitDict, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mstr\u001b[39m(key))\n\u001b[0;32m    398\u001b[0m \u001b[38;5;66;03m# 2nd case: Uses instructions: `info.splits['train[50%]']`\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 400\u001b[0m   instructions \u001b[38;5;241m=\u001b[39m \u001b[43m_make_file_instructions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m      \u001b[49m\u001b[43msplit_infos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[43m      \u001b[49m\u001b[43minstruction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    404\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m SubSplitInfo(instructions)\n",
      "File \u001b[1;32mc:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\tensorflow_datasets\\core\\splits.py:501\u001b[0m, in \u001b[0;36m_make_file_instructions\u001b[1;34m(split_infos, instruction)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns file instructions by applying the given instruction on the given splits.\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \n\u001b[0;32m    487\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;124;03m  List of FileInstruction instances\u001b[39;00m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;66;03m# TODO(epot): Should try to merge the instructions together as well as\u001b[39;00m\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# performing additional validation. For example, should raise an error\u001b[39;00m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;66;03m# if there is overlap between splits (`train[:50]+train[:25]`)\u001b[39;00m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;66;03m# If there is a single shard, `train[:25]+train[50:75]` could be optimized\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;66;03m# into a single `ds.take(25).skip(50-25).take(75-50)`\u001b[39;00m\n\u001b[1;32m--> 501\u001b[0m absolute_instructions \u001b[38;5;241m=\u001b[39m \u001b[43m_make_absolute_instructions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_infos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_infos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstruction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstruction\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m instructions \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    505\u001b[0m info_per_split \u001b[38;5;241m=\u001b[39m {split_info\u001b[38;5;241m.\u001b[39mname: split_info \u001b[38;5;28;01mfor\u001b[39;00m split_info \u001b[38;5;129;01min\u001b[39;00m split_infos}\n",
      "File \u001b[1;32mc:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\tensorflow_datasets\\core\\splits.py:455\u001b[0m, in \u001b[0;36m_make_absolute_instructions\u001b[1;34m(split_infos, instruction)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_make_absolute_instructions\u001b[39m(\n\u001b[0;32m    451\u001b[0m     split_infos: Iterable[SplitInfo],\n\u001b[0;32m    452\u001b[0m     instruction: SplitArg,\n\u001b[0;32m    453\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[_AbsoluteInstruction]:\n\u001b[0;32m    454\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(instruction, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 455\u001b[0m     instruction \u001b[38;5;241m=\u001b[39m \u001b[43mAbstractSplit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_spec\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstruction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    457\u001b[0m   \u001b[38;5;66;03m# Create the absolute instruction (per split)\u001b[39;00m\n\u001b[0;32m    458\u001b[0m   split_info_map \u001b[38;5;241m=\u001b[39m {split_info\u001b[38;5;241m.\u001b[39mname: split_info \u001b[38;5;28;01mfor\u001b[39;00m split_info \u001b[38;5;129;01min\u001b[39;00m split_infos}\n",
      "File \u001b[1;32mc:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\tensorflow_datasets\\core\\splits.py:552\u001b[0m, in \u001b[0;36mAbstractSplit.from_spec\u001b[1;34m(cls, spec)\u001b[0m\n\u001b[0;32m    547\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo instructions could be built out of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mtry_reraise(\n\u001b[0;32m    549\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError parsing split \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m. See format at: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.tensorflow.org/datasets/splits\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    551\u001b[0m ):\n\u001b[1;32m--> 552\u001b[0m   instructions \u001b[38;5;241m=\u001b[39m [_str_to_relative_instruction(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m subs]\n\u001b[0;32m    553\u001b[0m \u001b[38;5;66;03m# Merge all splits together (_SplitAll)\u001b[39;00m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m functools\u001b[38;5;241m.\u001b[39mreduce(operator\u001b[38;5;241m.\u001b[39madd, instructions)\n",
      "File \u001b[1;32mc:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\tensorflow_datasets\\core\\splits.py:552\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    547\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo instructions could be built out of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mtry_reraise(\n\u001b[0;32m    549\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError parsing split \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m. See format at: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.tensorflow.org/datasets/splits\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    551\u001b[0m ):\n\u001b[1;32m--> 552\u001b[0m   instructions \u001b[38;5;241m=\u001b[39m [\u001b[43m_str_to_relative_instruction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m subs]\n\u001b[0;32m    553\u001b[0m \u001b[38;5;66;03m# Merge all splits together (_SplitAll)\u001b[39;00m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m functools\u001b[38;5;241m.\u001b[39mreduce(operator\u001b[38;5;241m.\u001b[39madd, instructions)\n",
      "File \u001b[1;32mc:\\Users\\jeppe\\anaconda3\\a3\\envs\\portfolioJFR\\lib\\site-packages\\tensorflow_datasets\\core\\splits.py:685\u001b[0m, in \u001b[0;36m_str_to_relative_instruction\u001b[1;34m(spec)\u001b[0m\n\u001b[0;32m    680\u001b[0m err_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    681\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnrecognized split format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m. See format at \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    682\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.tensorflow.org/datasets/splits\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    683\u001b[0m )\n\u001b[0;32m    684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m res:\n\u001b[1;32m--> 685\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err_msg)\n\u001b[0;32m    686\u001b[0m split_name \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit_name\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    687\u001b[0m split_selector \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit_selector\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Error parsing split 'test[:2.5%]'. See format at: https://www.tensorflow.org/datasets/splits\nUnrecognized split format: 'test[:2.5%]'. See format at https://www.tensorflow.org/datasets/splits"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "def convert_sample(sample):\n",
    "    image, label = sample['image'], sample['label']  \n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    return image, image  # Use the image itself as the target label\n",
    "\n",
    "def map_func(sample):\n",
    "    image = sample['image']\n",
    "    label = sample['label']\n",
    "    label = tf.one_hot(label, 2, dtype=tf.float32)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "# Corrected file path with escaped backslashes and a missing comma\n",
    "ds1, ds2, ds3 = tfds.load('patch_camelyon',\n",
    "                         split=['train[:10%]', 'test[:2.5%]', 'validation[:2.5%]'],\n",
    "                         data_dir=r'C:\\Job_og_eksamensbevis\\Github\\projekter\\Projekt_1_Computer_Vision\\path_to_data_directory',\n",
    "                         download=False,\n",
    "                         shuffle_files=True)\n",
    "\n",
    "train_dataset       = ds1.map(convert_sample).batch(32)\n",
    "validation_dataset  = ds3.map(convert_sample).batch(32)\n",
    "test_dataset        = ds2.map(convert_sample).batch(32)\n",
    "\n",
    "train_dataset_cnn       = ds1.map(map_func).batch(32)\n",
    "validation_dataset_cnn  = ds3.map(map_func).batch(32)\n",
    "test_dataset_cnn        = ds2.map(map_func).batch(32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#baseline\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard, \n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Reshape, UpSampling2D, Conv2DTranspose \n",
    "\n",
    "def build_autoencoder(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_layer)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "    encoded_layer = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "    # Decoder\n",
    "    x = Conv2D(16, (3, 3), activation='relu', padding='same')(encoded_layer)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    decoded_layer = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    autoencoder = tf.keras.models.Model(inputs=input_layer, outputs=decoded_layer)\n",
    "\n",
    "    return autoencoder\n",
    "\n",
    "# Assuming input image shape is (96, 96, 3) for a 96x96px color image\n",
    "input_shape = (96, 96, 3)\n",
    "autoencoder = build_autoencoder(input_shape)\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "autoencoder.compile(optimizer='adam', loss='mse')  # You can use other loss functions depending on your task\n",
    "\n",
    "\n",
    "# Train the autoencoder\n",
    "history = autoencoder.fit(train_dataset,\n",
    "                          validation_data=validation_dataset,\n",
    "                          epochs=10,\n",
    "                          batch_size=32\n",
    "                          )\n",
    "\n",
    "\n",
    "# Evaluate performance (you may use a validation set or a separate test set)\n",
    "loss = autoencoder.evaluate(test_dataset, verbose=1)\n",
    "\n",
    "\n",
    "# Access the training history\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "\n",
    "# Plot the training progress\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jeppe\\OneDrive\\Dokumenter\\Privat\\SDU\\DataScience\\anaconda_spyder\\envs\\ENV_NAME\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jeppe\\OneDrive\\Dokumenter\\Privat\\SDU\\DataScience\\anaconda_spyder\\envs\\ENV_NAME\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jeppe\\OneDrive\\Dokumenter\\Privat\\SDU\\DataScience\\anaconda_spyder\\envs\\ENV_NAME\\lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jeppe\\OneDrive\\Dokumenter\\Privat\\SDU\\DataScience\\anaconda_spyder\\envs\\ENV_NAME\\lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jeppe\\OneDrive\\Dokumenter\\Privat\\SDU\\DataScience\\anaconda_spyder\\envs\\ENV_NAME\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jeppe\\OneDrive\\Dokumenter\\Privat\\SDU\\DataScience\\anaconda_spyder\\envs\\ENV_NAME\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\jeppe\\OneDrive\\Dokumenter\\Privat\\SDU\\DataScience\\anaconda_spyder\\envs\\ENV_NAME\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jeppe\\OneDrive\\Dokumenter\\Privat\\SDU\\DataScience\\anaconda_spyder\\envs\\ENV_NAME\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820/820 [==============================] - 119s 143ms/step - loss: 0.0195 - val_loss: 0.0146\n",
      "Epoch 2/10\n",
      "820/820 [==============================] - 114s 138ms/step - loss: 0.0132 - val_loss: 0.0125\n",
      "Epoch 3/10\n",
      "789/820 [===========================>..] - ETA: 4s - loss: 0.0120"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m autoencoder\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m)  \n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Train the autoencoder\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\n\u001b[0;32m     42\u001b[0m \u001b[43m                          \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Evaluate performance (you may use a validation set or a separate test set)\u001b[39;00m\n\u001b[0;32m     45\u001b[0m loss \u001b[38;5;241m=\u001b[39m autoencoder\u001b[38;5;241m.\u001b[39mevaluate(test_dataset, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jeppe\\OneDrive\\Dokumenter\\Privat\\SDU\\DataScience\\anaconda_spyder\\envs\\ENV_NAME\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\jeppe\\OneDrive\\Dokumenter\\Privat\\SDU\\DataScience\\anaconda_spyder\\envs\\ENV_NAME\\lib\\site-packages\\keras\\src\\engine\\training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1805\u001b[0m ):\n\u001b[0;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\jeppe\\OneDrive\\Dokumenter\\Privat\\SDU\\DataScience\\anaconda_spyder\\envs\\ENV_NAME\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\jeppe\\OneDrive\\Dokumenter\\Privat\\SDU\\DataScience\\anaconda_spyder\\envs\\ENV_NAME\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\jeppe\\OneDrive\\Dokumenter\\Privat\\SDU\\DataScience\\anaconda_spyder\\envs\\ENV_NAME\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\jeppe\\OneDrive\\Dokumenter\\Privat\\SDU\\DataScience\\anaconda_spyder\\envs\\ENV_NAME\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jeppe\\OneDrive\\Dokumenter\\Privat\\SDU\\DataScience\\anaconda_spyder\\envs\\ENV_NAME\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1325\u001b[0m     args,\n\u001b[0;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1327\u001b[0m     executing_eagerly)\n\u001b[0;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\jeppe\\OneDrive\\Dokumenter\\Privat\\SDU\\DataScience\\anaconda_spyder\\envs\\ENV_NAME\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\jeppe\\OneDrive\\Dokumenter\\Privat\\SDU\\DataScience\\anaconda_spyder\\envs\\ENV_NAME\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\jeppe\\OneDrive\\Dokumenter\\Privat\\SDU\\DataScience\\anaconda_spyder\\envs\\ENV_NAME\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1501\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\jeppe\\OneDrive\\Dokumenter\\Privat\\SDU\\DataScience\\anaconda_spyder\\envs\\ENV_NAME\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#preferred AE-model\n",
    "#AE med dropout\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Reshape, UpSampling2D, Dropout, Conv2DTranspose \n",
    "\n",
    "def build_autoencoder(input_shape, dropout_rate=0.2):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_layer)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "    encoded_layer = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "    # Decoder\n",
    "    x = Conv2D(16, (3, 3), activation='relu', padding='same')(encoded_layer)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Dropout(dropout_rate)(x)  # Dropout layer added here\n",
    "    decoded_layer = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    autoencoder = tf.keras.models.Model(inputs=input_layer, outputs=decoded_layer)\n",
    "\n",
    "    return autoencoder\n",
    "\n",
    "# Assuming input image shape is (96, 96, 3) for a 96x96px color image\n",
    "input_shape = (96, 96, 3)\n",
    "autoencoder = build_autoencoder(input_shape)\n",
    "dropout_rate = 0.2  # Modify the dropout rate as per your preference \n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3) \n",
    "\n",
    "# Compile the model\n",
    "autoencoder.compile(optimizer='adam', loss='mse')  \n",
    "\n",
    "# Train the autoencoder\n",
    "history = autoencoder.fit(train_dataset,\n",
    "                          validation_data=validation_dataset,\n",
    "                          epochs=10,\n",
    "                          batch_size=32\n",
    "                          )\n",
    "\n",
    "# Evaluate performance (you may use a validation set or a separate test set)\n",
    "loss = autoencoder.evaluate(test_dataset, verbose=1)\n",
    "\n",
    "\n",
    "# Access the training history\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "\n",
    "# Plot the training progress\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reconstruct images\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# Antal billeder at vise\n",
    "n = 10\n",
    "\n",
    "# Hent nogle eksempler fra testdatasttet\n",
    "samples = next(iter(test_dataset))[0][:n]  # Brug kun inputbilleder\n",
    "\n",
    "# Generer rekonstruerede billeder fra de originale billeder\n",
    "reconstructed_images = autoencoder.predict(samples)\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # Display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.title(\"Original\")\n",
    "    plt.imshow(tf.squeeze(samples[i]))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Display rekonstruktion\n",
    "    bx = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.title(\"Rekonstrueret\")\n",
    "    plt.imshow(tf.squeeze(reconstructed_images[i]))\n",
    "    plt.gray()\n",
    "    bx.get_xaxis().set_visible(False)\n",
    "    bx.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "\n",
    "# Assuming 'samples' contains original images and 'reconstructed_images' contains corresponding reconstructed images\n",
    "\n",
    "# Reshape the arrays to 2D (flatten each image)\n",
    "samples_flat = tf.reshape(samples, (samples.shape[0], -1))\n",
    "reconstructed_flat = tf.reshape(reconstructed_images, (reconstructed_images.shape[0], -1))\n",
    "\n",
    "# Convert EagerTensor to numpy arrays\n",
    "samples_flat = samples_flat.numpy()\n",
    "reconstructed_flat = reconstructed_flat.numpy()\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(samples_flat, reconstructed_flat)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "portfolioJFR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
